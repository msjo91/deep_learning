{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CIFAR-10\n",
    "def load_CIFAR10(pos, n_chunks=1):\n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for i in range(n_chunks):\n",
    "        train = unpickle(pos + '/data_batch_{0}'.format(i + 1))\n",
    "        Xtr.extend(train[b'data'])\n",
    "        Ytr.extend(train[b'labels'])\n",
    "        test = unpickle(pos + '/test_batch')\n",
    "        Xte = test[b'data']\n",
    "        Yte = test[b'labels']\n",
    "    return np.array(Xtr), np.array(Ytr), np.array(Xte), np.array(Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Ytr, Yte):\n",
    "    Ytr_onehot = np.zeros((Ytr.size, 10))\n",
    "    Yte_onehot = np.zeros((Yte.size, 10))\n",
    "    for i in range(Ytr.size):\n",
    "        Ytr_onehot[i][Ytr[i]] = 1\n",
    "    for i in range(Yte.size):\n",
    "        Yte_onehot[i][Yte[i]] = 1\n",
    "    return Ytr_onehot, Yte_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and test data\n",
    "Xtr, Ytr, Xte, Yte = load_CIFAR10('cifar-10-batches-py', 5)\n",
    "                                 \n",
    "# image data, each data size is 32*32*3\n",
    "Xtr = Xtr.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "Xte= Xte.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "\n",
    "# label data of train and test data, label data is represented by one-hot encoding\n",
    "Ytr_onehot, Yte_onehot = onehot_encoding(Ytr, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "    dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the layers of CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv-1'):\n",
    "    W1 = tf.Variable(tf.random_normal([3, 3, 3, 32], stddev=0.01))\n",
    "    # padding='SAME' keeps output size equal to input\n",
    "    L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # Batch normalization\n",
    "    batch_mean, batch_var = tf.nn.moments(L1, [0])\n",
    "    scale = tf.Variable(tf.ones([32, 32, 32]))\n",
    "    beta = tf.Variable(tf.zeros([32, 32, 32]))\n",
    "    L1 = tf.nn.batch_normalization(L1, batch_mean, batch_var, beta, scale, 1e-3)\n",
    "    L1 = tf.nn.relu(L1)\n",
    "    # Pooling layer\n",
    "    L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv-2'):\n",
    "    W2 = tf.Variable(tf.random_normal([3, 3, 32, 128], stddev=0.01))\n",
    "    L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    batch_mean, batch_var = tf.nn.moments(L2, [0])\n",
    "    scale = tf.Variable(tf.ones([16, 16, 128]))\n",
    "    beta = tf.Variable(tf.zeros([16, 16, 128]))\n",
    "    L2 = tf.nn.batch_normalization(L2, batch_mean, batch_var, beta, scale, 1e-3)\n",
    "\n",
    "    L2 = tf.nn.relu(L2)\n",
    "    L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Conv-3'):\n",
    "    W3 = tf.Variable(tf.random_normal([5, 5, 128, 256], stddev=0.01))\n",
    "    L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    batch_mean, batch_var = tf.nn.moments(L3, [0])\n",
    "    scale = tf.Variable(tf.ones([8, 8, 256]))\n",
    "    beta = tf.Variable(tf.zeros([8, 8, 256]))\n",
    "    L3 = tf.nn.batch_normalization(L3, batch_mean, batch_var, beta, scale, 1e-3)\n",
    "\n",
    "    L3 = tf.nn.relu(L3)\n",
    "    L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Full-1'):\n",
    "    W5 = tf.Variable(tf.random_normal([4 * 4 * 256, 512], stddev=0.01))\n",
    "    L5 = tf.reshape(L3, [-1, 4 * 4 * 256])\n",
    "    L5 = tf.matmul(L5, W5)\n",
    "    batch_mean, batch_var = tf.nn.moments(L5, [0])\n",
    "    scale = tf.Variable(tf.ones([512]))\n",
    "    beta = tf.Variable(tf.zeros([512]))\n",
    "    L5 = tf.nn.batch_normalization(L5, batch_mean, batch_var, beta, scale, 1e-3)\n",
    "    L5 = tf.nn.relu(L5)\n",
    "    # Dropout\n",
    "    L5 = tf.nn.dropout(L5, dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Full-2'):\n",
    "    W6 = tf.Variable(tf.random_normal([512, 10], stddev=0.01))\n",
    "    model = tf.matmul(L5, W6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function, you can change the implementation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001, 0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement train and test processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = 100\n",
    "total_batch = int(len(Xtr) / mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(start, end):\n",
    "    batch_xs = Xtr[start:end]\n",
    "    batch_ys = Ytr_onehot[start:end]\n",
    "    return batch_xs, batch_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01 \tAvg cost: 1.124 \tTest accuracy:  0.7047\n",
      "Epoch:  02 \tAvg cost: 0.758 \tTest accuracy:  0.7445\n",
      "Epoch:  03 \tAvg cost: 0.576 \tTest accuracy:  0.7553\n",
      "Epoch:  04 \tAvg cost: 0.434 \tTest accuracy:  0.7586\n",
      "Epoch:  05 \tAvg cost: 0.308 \tTest accuracy:  0.7614\n",
      "Epoch:  06 \tAvg cost: 0.221 \tTest accuracy:  0.7608\n",
      "Epoch:  07 \tAvg cost: 0.157 \tTest accuracy:  0.7599\n",
      "Epoch:  08 \tAvg cost: 0.119 \tTest accuracy:  0.7605\n",
      "Epoch:  09 \tAvg cost: 0.103 \tTest accuracy:  0.759\n",
      "Epoch:  10 \tAvg cost: 0.084 \tTest accuracy:  0.7635\n",
      "Epoch:  11 \tAvg cost: 0.068 \tTest accuracy:  0.7747\n",
      "Epoch:  12 \tAvg cost: 0.058 \tTest accuracy:  0.7704\n",
      "Epoch:  13 \tAvg cost: 0.054 \tTest accuracy:  0.77\n",
      "Epoch:  14 \tAvg cost: 0.054 \tTest accuracy:  0.7677\n",
      "Epoch:  15 \tAvg cost: 0.056 \tTest accuracy:  0.7636\n",
      "Epoch:  16 \tAvg cost: 0.046 \tTest accuracy:  0.7675\n",
      "Epoch:  17 \tAvg cost: 0.041 \tTest accuracy:  0.7703\n",
      "Epoch:  18 \tAvg cost: 0.038 \tTest accuracy:  0.7652\n",
      "Epoch:  19 \tAvg cost: 0.045 \tTest accuracy:  0.769\n",
      "Epoch:  20 \tAvg cost: 0.037 \tTest accuracy:  0.7703\n",
      "Epoch:  21 \tAvg cost: 0.035 \tTest accuracy:  0.772\n",
      "Epoch:  22 \tAvg cost: 0.036 \tTest accuracy:  0.7724\n",
      "Epoch:  23 \tAvg cost: 0.033 \tTest accuracy:  0.7648\n",
      "Epoch:  24 \tAvg cost: 0.028 \tTest accuracy:  0.7698\n",
      "Epoch:  25 \tAvg cost: 0.033 \tTest accuracy:  0.7693\n",
      "Epoch:  26 \tAvg cost: 0.029 \tTest accuracy:  0.7693\n",
      "Epoch:  27 \tAvg cost: 0.027 \tTest accuracy:  0.7739\n",
      "Epoch:  28 \tAvg cost: 0.028 \tTest accuracy:  0.7681\n",
      "Epoch:  29 \tAvg cost: 0.026 \tTest accuracy:  0.7739\n",
      "Epoch:  30 \tAvg cost: 0.020 \tTest accuracy:  0.769\n",
      "Epoch:  31 \tAvg cost: 0.023 \tTest accuracy:  0.7701\n",
      "Epoch:  32 \tAvg cost: 0.029 \tTest accuracy:  0.7666\n",
      "Epoch:  33 \tAvg cost: 0.021 \tTest accuracy:  0.774\n",
      "Epoch:  34 \tAvg cost: 0.020 \tTest accuracy:  0.7738\n",
      "Epoch:  35 \tAvg cost: 0.022 \tTest accuracy:  0.7691\n",
      "Epoch:  36 \tAvg cost: 0.019 \tTest accuracy:  0.7692\n",
      "Epoch:  37 \tAvg cost: 0.023 \tTest accuracy:  0.7739\n",
      "Epoch:  38 \tAvg cost: 0.019 \tTest accuracy:  0.7755\n",
      "Epoch:  39 \tAvg cost: 0.021 \tTest accuracy:  0.7722\n",
      "Epoch:  40 \tAvg cost: 0.019 \tTest accuracy:  0.7733\n",
      "Epoch:  41 \tAvg cost: 0.015 \tTest accuracy:  0.7748\n",
      "Epoch:  42 \tAvg cost: 0.017 \tTest accuracy:  0.7696\n",
      "Epoch:  43 \tAvg cost: 0.020 \tTest accuracy:  0.7709\n",
      "Epoch:  44 \tAvg cost: 0.016 \tTest accuracy:  0.7765\n",
      "Epoch:  45 \tAvg cost: 0.016 \tTest accuracy:  0.7656\n",
      "Epoch:  46 \tAvg cost: 0.024 \tTest accuracy:  0.7768\n",
      "Epoch:  47 \tAvg cost: 0.016 \tTest accuracy:  0.779\n",
      "Epoch:  48 \tAvg cost: 0.011 \tTest accuracy:  0.7817\n",
      "Epoch:  49 \tAvg cost: 0.012 \tTest accuracy:  0.7804\n",
      "Epoch:  50 \tAvg cost: 0.019 \tTest accuracy:  0.7744\n",
      "Epoch:  51 \tAvg cost: 0.017 \tTest accuracy:  0.7762\n",
      "Epoch:  52 \tAvg cost: 0.014 \tTest accuracy:  0.7729\n",
      "Epoch:  53 \tAvg cost: 0.014 \tTest accuracy:  0.7775\n",
      "Epoch:  54 \tAvg cost: 0.013 \tTest accuracy:  0.776\n",
      "Epoch:  55 \tAvg cost: 0.011 \tTest accuracy:  0.7746\n",
      "Epoch:  56 \tAvg cost: 0.013 \tTest accuracy:  0.7674\n",
      "Epoch:  57 \tAvg cost: 0.015 \tTest accuracy:  0.7747\n",
      "Epoch:  58 \tAvg cost: 0.015 \tTest accuracy:  0.7712\n",
      "Epoch:  59 \tAvg cost: 0.015 \tTest accuracy:  0.7693\n",
      "Epoch:  60 \tAvg cost: 0.013 \tTest accuracy:  0.777\n",
      "Epoch:  61 \tAvg cost: 0.013 \tTest accuracy:  0.7761\n",
      "Epoch:  62 \tAvg cost: 0.013 \tTest accuracy:  0.7749\n",
      "Epoch:  63 \tAvg cost: 0.014 \tTest accuracy:  0.7729\n",
      "Epoch:  64 \tAvg cost: 0.010 \tTest accuracy:  0.7754\n",
      "Epoch:  65 \tAvg cost: 0.013 \tTest accuracy:  0.774\n",
      "Epoch:  66 \tAvg cost: 0.013 \tTest accuracy:  0.7762\n",
      "Epoch:  67 \tAvg cost: 0.014 \tTest accuracy:  0.7744\n",
      "Epoch:  68 \tAvg cost: 0.008 \tTest accuracy:  0.7716\n",
      "Epoch:  69 \tAvg cost: 0.012 \tTest accuracy:  0.7739\n",
      "Epoch:  70 \tAvg cost: 0.011 \tTest accuracy:  0.7734\n",
      "Epoch:  71 \tAvg cost: 0.010 \tTest accuracy:  0.7702\n",
      "Epoch:  72 \tAvg cost: 0.010 \tTest accuracy:  0.7737\n",
      "Epoch:  73 \tAvg cost: 0.013 \tTest accuracy:  0.7735\n",
      "Epoch:  74 \tAvg cost: 0.010 \tTest accuracy:  0.7738\n",
      "Epoch:  75 \tAvg cost: 0.011 \tTest accuracy:  0.7743\n",
      "Epoch:  76 \tAvg cost: 0.008 \tTest accuracy:  0.7764\n",
      "Epoch:  77 \tAvg cost: 0.012 \tTest accuracy:  0.7737\n",
      "Epoch:  78 \tAvg cost: 0.009 \tTest accuracy:  0.775\n",
      "Epoch:  79 \tAvg cost: 0.012 \tTest accuracy:  0.7758\n",
      "Epoch:  80 \tAvg cost: 0.008 \tTest accuracy:  0.7754\n",
      "Epoch:  81 \tAvg cost: 0.010 \tTest accuracy:  0.7697\n",
      "Epoch:  82 \tAvg cost: 0.009 \tTest accuracy:  0.7748\n",
      "Epoch:  83 \tAvg cost: 0.012 \tTest accuracy:  0.7761\n",
      "Epoch:  84 \tAvg cost: 0.007 \tTest accuracy:  0.7817\n",
      "Epoch:  85 \tAvg cost: 0.008 \tTest accuracy:  0.7724\n",
      "Epoch:  86 \tAvg cost: 0.013 \tTest accuracy:  0.7738\n",
      "Epoch:  87 \tAvg cost: 0.009 \tTest accuracy:  0.7737\n",
      "Epoch:  88 \tAvg cost: 0.008 \tTest accuracy:  0.7762\n",
      "Epoch:  89 \tAvg cost: 0.009 \tTest accuracy:  0.7762\n",
      "Epoch:  90 \tAvg cost: 0.008 \tTest accuracy:  0.7784\n",
      "Epoch:  91 \tAvg cost: 0.008 \tTest accuracy:  0.7744\n",
      "Epoch:  92 \tAvg cost: 0.010 \tTest accuracy:  0.7725\n",
      "Epoch:  93 \tAvg cost: 0.010 \tTest accuracy:  0.773\n",
      "Epoch:  94 \tAvg cost: 0.009 \tTest accuracy:  0.7771\n",
      "Epoch:  95 \tAvg cost: 0.006 \tTest accuracy:  0.78\n",
      "Epoch:  96 \tAvg cost: 0.008 \tTest accuracy:  0.7765\n",
      "Epoch:  97 \tAvg cost: 0.007 \tTest accuracy:  0.7692\n",
      "Epoch:  98 \tAvg cost: 0.008 \tTest accuracy:  0.7693\n",
      "Epoch:  99 \tAvg cost: 0.008 \tTest accuracy:  0.7721\n",
      "Epoch:  100 \tAvg cost: 0.010 \tTest accuracy:  0.7681\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    total_cost = 0\n",
    "    \n",
    "    batch_start = 0\n",
    "    batch_end = mini_batch\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = next_batch(batch_start, batch_end)\n",
    "#         batch_xs = batch_xs.reshape(-1, 3136, 3136, 1)    # When image number is unknown put -1\n",
    "        _, curr_loss, = sess.run([optimizer, cost],\n",
    "                                 feed_dict={X: batch_xs, Y: batch_ys, dropout_prob: 0.7})\n",
    "        total_cost += curr_loss\n",
    "        batch_start += mini_batch\n",
    "        batch_end += mini_batch\n",
    "\n",
    "    correctness = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32))\n",
    "    print('Epoch: ', '%02d' % (epoch + 1),\n",
    "          '\\tAvg cost: {:,.3f}'.format(total_cost / total_batch),\n",
    "          '\\tTest accuracy: ', sess.run(accuracy, feed_dict={X: Xte,\n",
    "                                                             Y: Yte_onehot,\n",
    "                                                             dropout_prob: 1.0}))    # No dropou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
