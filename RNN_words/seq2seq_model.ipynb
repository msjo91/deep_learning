{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "num_dic = {v: i for i, v in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = [['word', '단어'], ['wood', '나무'], ['game', '놀이'],\n",
    "            ['girl', '소녀'], ['kiss', '키스'], ['love', '사랑']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        inpt = [num_dic[w] for w in seq[0]]\n",
    "        output = [num_dic[w] for w in ('S' + seq[1])]\n",
    "        target = [num_dic[w] for w  in (seq[1] + 'E')]\n",
    "        # Each number from num_dic becomes an one-hot vector\n",
    "        input_batch.append(np.eye(dic_len)[inpt]) # Create identity matrix\n",
    "        output_batch.append(np.eye(dic_len)[output]) # Create identity matrix \n",
    "        target_batch.append(target)\n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_hidden = 128 # Number of hidden states\n",
    "total_epoch = 100\n",
    "n_input = n_class = dic_len # Number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "targets = tf.placeholder(tf.int64, [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(\n",
    "        dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets)\n",
    ")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, cost = 3.703137\n",
      "Epoch: 002, cost = 2.751431\n",
      "Epoch: 003, cost = 1.714648\n",
      "Epoch: 004, cost = 0.829796\n",
      "Epoch: 005, cost = 0.826042\n",
      "Epoch: 006, cost = 0.396648\n",
      "Epoch: 007, cost = 0.289734\n",
      "Epoch: 008, cost = 0.195465\n",
      "Epoch: 009, cost = 0.155327\n",
      "Epoch: 010, cost = 0.085273\n",
      "Epoch: 011, cost = 0.067709\n",
      "Epoch: 012, cost = 0.051858\n",
      "Epoch: 013, cost = 0.057874\n",
      "Epoch: 014, cost = 0.021420\n",
      "Epoch: 015, cost = 0.034621\n",
      "Epoch: 016, cost = 0.028905\n",
      "Epoch: 017, cost = 0.005745\n",
      "Epoch: 018, cost = 0.007259\n",
      "Epoch: 019, cost = 0.013755\n",
      "Epoch: 020, cost = 0.014086\n",
      "Epoch: 021, cost = 0.022612\n",
      "Epoch: 022, cost = 0.002314\n",
      "Epoch: 023, cost = 0.003651\n",
      "Epoch: 024, cost = 0.035566\n",
      "Epoch: 025, cost = 0.001597\n",
      "Epoch: 026, cost = 0.006961\n",
      "Epoch: 027, cost = 0.003130\n",
      "Epoch: 028, cost = 0.002283\n",
      "Epoch: 029, cost = 0.003220\n",
      "Epoch: 030, cost = 0.004545\n",
      "Epoch: 031, cost = 0.000992\n",
      "Epoch: 032, cost = 0.002446\n",
      "Epoch: 033, cost = 0.002930\n",
      "Epoch: 034, cost = 0.001522\n",
      "Epoch: 035, cost = 0.000964\n",
      "Epoch: 036, cost = 0.002720\n",
      "Epoch: 037, cost = 0.002551\n",
      "Epoch: 038, cost = 0.001185\n",
      "Epoch: 039, cost = 0.000747\n",
      "Epoch: 040, cost = 0.002302\n",
      "Epoch: 041, cost = 0.001017\n",
      "Epoch: 042, cost = 0.003755\n",
      "Epoch: 043, cost = 0.001366\n",
      "Epoch: 044, cost = 0.000561\n",
      "Epoch: 045, cost = 0.000744\n",
      "Epoch: 046, cost = 0.000992\n",
      "Epoch: 047, cost = 0.000485\n",
      "Epoch: 048, cost = 0.000861\n",
      "Epoch: 049, cost = 0.000499\n",
      "Epoch: 050, cost = 0.001711\n",
      "Epoch: 051, cost = 0.000204\n",
      "Epoch: 052, cost = 0.000913\n",
      "Epoch: 053, cost = 0.000500\n",
      "Epoch: 054, cost = 0.000348\n",
      "Epoch: 055, cost = 0.000506\n",
      "Epoch: 056, cost = 0.000624\n",
      "Epoch: 057, cost = 0.001161\n",
      "Epoch: 058, cost = 0.000353\n",
      "Epoch: 059, cost = 0.001673\n",
      "Epoch: 060, cost = 0.000629\n",
      "Epoch: 061, cost = 0.000224\n",
      "Epoch: 062, cost = 0.001015\n",
      "Epoch: 063, cost = 0.000243\n",
      "Epoch: 064, cost = 0.000433\n",
      "Epoch: 065, cost = 0.000225\n",
      "Epoch: 066, cost = 0.000616\n",
      "Epoch: 067, cost = 0.001935\n",
      "Epoch: 068, cost = 0.000387\n",
      "Epoch: 069, cost = 0.000332\n",
      "Epoch: 070, cost = 0.000476\n",
      "Epoch: 071, cost = 0.000201\n",
      "Epoch: 072, cost = 0.000448\n",
      "Epoch: 073, cost = 0.000627\n",
      "Epoch: 074, cost = 0.000188\n",
      "Epoch: 075, cost = 0.000232\n",
      "Epoch: 076, cost = 0.000216\n",
      "Epoch: 077, cost = 0.000392\n",
      "Epoch: 078, cost = 0.000132\n",
      "Epoch: 079, cost = 0.000169\n",
      "Epoch: 080, cost = 0.000494\n",
      "Epoch: 081, cost = 0.000168\n",
      "Epoch: 082, cost = 0.000313\n",
      "Epoch: 083, cost = 0.000485\n",
      "Epoch: 084, cost = 0.000467\n",
      "Epoch: 085, cost = 0.000195\n",
      "Epoch: 086, cost = 0.000153\n",
      "Epoch: 087, cost = 0.000532\n",
      "Epoch: 088, cost = 0.000516\n",
      "Epoch: 089, cost = 0.000295\n",
      "Epoch: 090, cost = 0.000281\n",
      "Epoch: 091, cost = 0.002367\n",
      "Epoch: 092, cost = 0.000332\n",
      "Epoch: 093, cost = 0.000509\n",
      "Epoch: 094, cost = 0.001407\n",
      "Epoch: 095, cost = 0.000253\n",
      "Epoch: 096, cost = 0.000164\n",
      "Epoch: 097, cost = 0.000147\n",
      "Epoch: 098, cost = 0.000129\n",
      "Epoch: 099, cost = 0.000142\n",
      "Epoch: 100, cost = 0.000252\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={enc_input: input_batch,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    print('Epoch: {:03}, cost = {:.6f}'.format(epoch + 1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(word):\n",
    "    seq_data = [word, 'P' * len(word)]\n",
    "    i_batch, o_batch, t_batch = make_batch([seq_data])\n",
    "    \n",
    "    prediction = tf.argmax(model, 2)\n",
    "    result = sess.run(prediction,\n",
    "                      feed_dict={enc_input: i_batch,\n",
    "                                 dec_input: o_batch,\n",
    "                                 targets: t_batch})\n",
    "    \n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ->  단어\n",
      "wodr ->  단어\n",
      "love ->  사랑\n",
      "loev ->  사랑\n",
      "abcd ->  사랑\n"
     ]
    }
   ],
   "source": [
    "print('word -> ', translate('word'))\n",
    "print('wodr -> ', translate('wodr'))\n",
    "print('love -> ', translate('love'))\n",
    "print('loev -> ', translate('loev'))\n",
    "print('abcd -> ', translate('abcd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: Having a small toy dataset will only provide low accuracy results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
